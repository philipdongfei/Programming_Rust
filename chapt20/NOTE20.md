# Asynchronous Programming

You can use Rust *asynchronous tasks* to interleave many independent activities on a single thread or a pool of worker threads. Asynchronous tasks are similar to threads, but are much quicker to create, pass control amongst themselves more efficiently, and have memory overhead an order of magnitude less than that of a thread.


## From Synchronous to Asynchronous

While this function is waiting for the system calls to return, its single thread is blocked: it can't do anything else until the sytem call finishes.

### Futures

Rust's approach to supporting asynchronous operations is to introduce a trait, **std::future::Future**:

    trait Future { 
        type Output;
        // For now, read `Pin<&mut Self>` as `&mut Self`.
        fn poll(self: Pin<&mut Self>, cx: &mut Context<'_>) 
                -> Poll<Self::Output>;
    }

    enum Poll<T> {
        Ready(T),
        Pending,
    }

A **Future** represents an operation that you can test for completion. A future's poll method never waits for the operation to finish: it always returns immediately. If the operation is complete, **poll** return **Poll::Ready(output)**, where **output** is its final result. Otherwise, it returns **Pending**. If and when the future is worth polling again, it promises to let us know by invoking a *waker*, a callback function supplied in the **Context**.

So an asynchronous version of **read_to_string** would have a signature roughly like this: 

    fn read_to_string(&mut self, buf: &mut String)
        -> impl Future<Output = Result<usize>>;

    fn read_to_string<'a>(&'a mut self, buf: &'a mut String)
        -> impl Future<Output = Result<usize>> + 'a;

One of the rules of the **Future** trait is that, once a future has returned **Poll::Ready**, it may assume it will never be polled again. Some futures just return **Poll::Pending** forever if they are overpolled; others may panic or hang.


### Async Functions and Await Expressions


    use async_std::io::prelude::*;
    use async_std::net;
    
    async fn cheapo_request(host: &str, port: u16, path: &str)
                                -> std::io::Result<String>
    {
        let mut socket = net::TcpStream::connect((host, port)).await?;
    
        let request = format!("GET {} HTTP/1.1\r\nHost: {}\r\n\r\n", path, host);
        socket.write_all(request.as_bytes()).await?;
        socket.shutdown(net::Shutdown::Write)?;
    
        let mut response = String::new();
        socket.read_to_string(&mut response).await?;
    
        Ok(response)
    }

This is token for token the same as our original version, except:

* The function starts with **async fn** instead of **fn**.
* It uses the **async_std** crate's asynchronous versions of **TcpStream::connect**, **write_all**, and **read_to_string**. These all return futures of their results. 
* After each call that returns a future, the code says **.await**. Although this looks like a reference to a struct field named **await**, it is actually special syntax built into the language for waiting until a future is ready. An **await** expression evaluates to the final value of the future. This is how the function obtains the results from **connect**, **write_all**, and **read_to_string**.

The future returned by an async function wraps up all the information the function body will need to run: the function's arguments, space for its local variables, and so on.
The future's specific type is generated automatically by the compiler, based on the function's body and arguments. This type doesn't have a name; all you know about it is that it implements **Future<Output=R>**, where **R** is the async function's return type. In this sense, futures of asynchronous functions are like closures: closures also have anonymous types, generated by the compiler, that implement the **FnOnce**, **Fn**, and **FnMut** traits.
When you first poll the future returned by **cheapo_request**, execution begins at the top of the function body and runs until the first **await** of the future returned by **TcpStream::connect**. The **await** expression polls the **connect** future, and if it is not ready, then it returns **Poll::Pending** to its own caller: polling **cheapo_request**'s future cannot preceed past that first **await** until a poll of **TcpStream::connect**'s future returns **Poll::Ready**. So a
rough equivalent of the expression **TcpStream::connect(...).await** might be:

    {
        // Note: this is pseudocode, not valid Rust
        let connect_future = TcpStream::connect(...);
        'retry_point:
        match connect_future.poll(cx) {
            Poll::Ready(value) => value,
            Poll::Pending => {
                // Arrrange for the next `poll` of `cheapo_request`'s
                // future to resume execution at 'retry_point.
                ...
                return Poll::Pending
            }
        }

    }

An **await** expression takes ownership of the future and then polls it. If it's ready, then the future's final value is the value of the **await** expression, and execution continues. Otherwise, it returns the **Poll::Pending** to its own caller.
But crucially, the next poll of cheapo_request's future doesn't start at the top of the function again: instead, it *resumes* execution mid-function at the point where it is about to poll **connect_future**. We don't progress to the rest of the async function until that future is ready.
As **cheapo_request**'s future continues to be polled, it will work its way through the function body from one **await** to the next, moving on only when the subfuture it's awaiting is ready. Thus, how many times **cheapo_request**'s future must be polled depends on both the behavior of the subfutures and the function's own control flow. **cheapo_request**'s future tracks the point at which the next **poll** should resume, and all the local state--variables, arguments,
temporaries--that resumption will need.
The ability to suspend execution mid-function and then resume later is unique to async functions.



### Calling Async Functions from Synchronous Code: block_on

We can call **cheapo_request** from an ordinary, synchronous function (like main, for example) using **async_std's task::block_on** function, which takes a future and polls it until it produces a value:


    fn main() -> std::io::Result<()> {
        use async_std::task;
    
        let response = task::block_on(cheapo_request("www.rust-lang.com", 80, "/"))?;
        println!("{}", response);
        Ok(())
    }

Since **block_on** is a synchronous function that produces the final value of an asynchronous function, you can think of it as an adapter from the asynchronous world to the synchronous world. But its blocking character also means that you should never use **block_on** within an async function: it would block the entire thread until the value is ready. Use **await** instead.

This is a good opportunity to walk through exactly what's going on in ordinary asynchronous execution:

* First, **main** calls **cheapo_request**, which returns future A of its final result. Then **main** passes that future to **async_std::block_on**, which polls it.
* Polling future A allows the body of **cheapo_request** to begin execution. It calls **TcpStream::connect** to obtain a future B of a socket and then awaits that. More precisely, since **TcpStream::connect** might encounter an error, B is a future of a **Result<TcpStream, std::io::Error>**.
* Future B gets polled by the **await**. Since the network connection is not yet established, B.poll returns **Poll::Pending**, but arranges to wake up the calling task once the socket is ready.
* Since future B wasn't ready, **A.poll** returns **Poll::Pending** to its own caller, **block_on**.
* Since **block_on** has nothing better to do, it goes to sleep. The entire thread is blocked now.
* When B's connection is ready to use, it wakes up the task that polled it. This stirs **block_on** into action, and it tries polling the future A again.
* Polling A causes **cheapo_request** to resume in its first **await**, where it polls B again.
* This time, B is ready: socket creation is complete, so it returns **Poll::Ready(Ok(socket))** to **A.poll**.
* The asynchronous call to **TcpStream::connect** is now complete. The value of the **TcpStream::connect(...).await** expression is thus **Ok(socket)**.
* The execution of **cheapo_request**'s body proceeds normally, building the request string using the **format!** macro and passing it to **socket.write_all**.
* Since **socket.write_all** is an asynchronous function, it returns a future C of its result, which **cheapo_request** duly awaits.

The rest of the story is similar.
It doesn't sound too hard to just write a loop that calls **poll** over and over. But what makes **async_std::task::block_on** valuable is that it knows how to go to sleep until the future is actually worth polling again, rather than wasting your processor time and battery life making billions of fruitless **poll** calls. The futures returned by basic I/O functions like **connect** and **read_to_string** retain the waker supplied by the **Context** passed to **poll** and invoke it when
**block_on** should wake up and try polling again.


### Spawning Async Tasks

the goal of this chapter is to get the thread *doing other work* while it's waiting.
For this, you can use **async_std::task::spawn_local**. This function takes a future and adds it to a pool that **block_on** will try polling whenever the future it's blocking on isn't ready.
The **spawn_local** function is an asynchronous analogue of the standard library's **std::thread::spawn** function for starting threads:

* **std::thread::spawn(c)** takes a closure **c** and starts a thread running it, returning a **std::thread::JoinHandle** whose **join** method waits for the thread to finish and returns whatever **c** returned.
* **async_std::task::spawn_local(f)** takes the future **f** and adds it to the pool to be polled when the current thread calls **block_on.spawn_local** returns its own **async_std::task::JoinHandle** type, itself a future that you can await to retrieve **f'**s final value.

        pub async fn many_requests(requests: Vec<(String, u16, String)>)
                            -> Vec<std::io::Result<String>>
        {
            use async_std::task;

            let mut handles = vec![];
            for (host, port, path) in requests {
                handles.push(task::spawn_local(cheapo_request(&host, port, &path)));
            }
            
            let mut results = vec![];
            for handle in handles {
                results.push(handle.await);
            }

            results

        }


All this execution takes place on a single thread, the three calls to **cheapo_request** being interleaved with each other through successive polls of their futures. An asynchronous call offers the appearance of a single function call running to completion, but this asynchronous call is realized by a series of synchronous calls to the future's **poll** method. Each individual **poll** call returns quickly, yielding the thread so that another async call can take a turn.

We have finally achieved the goal we set out at the beginning of the chapter: letting a thread take on other work while it waits for I/O to complete so that the thread's resources aren't tied up doing nothing. Even better, this goal was met with code that looks very much like ordinary Rust code: some of the functions are marked **async**, some of the function calls are followed by **.await**, and we use functions from **async_std** instead fo **std**, but otherwise,
it's ordinary Rust code.
One important difference to keep in mind between asynchronous tasks and threads is that switching from one async task to another happens only at **await** expressions, when the future being awaited returns **Poll::Pending**. This means that if you put a long-running computation in **cheapo_request**, none of the other tasks you passed to **spawn_local** will get a chance to run until it's done. With threads, this problem doesn't arise: the operating system can suspend
any thread at any point and sets timers to ensure that no thread monopolizes the processor. Asynchronous code depends on the willing cooperation of the futures sharing the thread.

### Async Blocks

Async blocks provide a concise way to separate out a section of code you'd like to run asynchronously. For example, in the previous section, **spawn_local** wrapper function to give us a future that took ownership of its arguments. You can get the same effect without the distraction of a wrapper function simply by calling **cheapo_request** from an async block:

    pub async fn many_requests(requests: Vec<(String, u16, String)>)
                    -> Vec<std::io::Result<String>>
    {
        use async_std::task;

        let mut handles = vec![];
        for (host, port, path) in requests {
            handles.push(task::spawn_local(async move {
                cheapo_request(&host, port, &path).await
            }));
        }
        ...
    }

Since this is an **async move** block, its future takes ownership of the **String** values **host** and **path**, just the way a **move** closure would. It then passes references to **cheapo_request**. The borrow checker can see that the block's **await** expression takes ownership of **cheapo_request**'s future, so the references to **host** and **path** cannot outlive the captured variables they borrow. The async block accomplishes the same thing as
**cheapo_owning_request**, but with less boilerplate.

In addition to asynchronous functions, Rust also supports *asynchronous blocks*. Whereas an ordinary block statement returns the value of its last expression, an async block returns *a future of* the value its last expression. You can use **await** expressions within an async block.


### Building Async Functions from Async Blocks

Asynchronous blocks give us another way to get the same effect as an Asynchronous function, with a little more flexibility. For example, we could write our **cheapo_request** example as an ordinary, synchronous function that returns the future of an async block(*building_async_func_from_async_blocks* fold):

    use std::io;
    use std::future::Future;

    fn cheapo_request<'a>(host: &'a str, port: u16, path: &'a str)
        -> impl Future<Output = io::Result<String>> + 'a
    {
        async move {
            ... function body ...
        }
    }

This second approach can be useful when you want to do some computation immediately when the function is called, before creating the future of its result. For example, yet another way to reconcile **cheapo_request** with **spawn_local** would be to make it into a synchronous function returning a 'static future that captures fully owned copies of its arguments(*building_async_func_from_async_blocks2* fold):

    fn cheapo_request(host: &str, port: u16, path: &str)
        -> impl Future<Output = io::Result<String>> + 'static
        {
            let host = host.to_string();
            let path = path.to_string();

            async move {
                ... use &*host, port, and path ...
            }
        }

Since this version of **cheapo_request** returns futures that are 'static, we can pass them directly to **spawn_local**: 

    let join_handle = async_std::task::spawn_local(
        cheapo_request("areweasyncyet.rs", 80, "/")
    );
    
    ... other work ...
    
    let response = join_handle.await?;

### Spawning Async Tasks on a Thread Pool

When you have enough computation to do that a single processor can't keep up, you can use **async_std::task::spawn** to spawn a future onto a pool of worker threads dedicated to polling futures that are ready to make progress.

    use async_std::task;

    let mut handles = vec![];
    for (host, port, path) in requests {
        // use async_std::task::spawn to spawn a future onto a pool of worker threads 
        // dedicated to polling futures that are ready to make progress. 
        handles.push(task::spawn(async move {
            cheapo_request(&host, port, &path).await
        }));
    }


Like **spawn_local**, **spawn** returns a **JoinHandle** value you can await to get the future's final value. But unlike **spawn_local**, the future doesn't have to wait for you to call **block_on** before it gets polled. As soon as one of the threads from the thread pool is free, it will try polling it.


### But Does Your Future Implement Send?

There is one restriction **spawn** imposes that **spawn_local** does not. Since the future is being sent off to another thread to run, the future must implement the **Send** marker trait. A future is **Send** only if all the values it contains are **Send**: all the function arguments, local variables, and even anonymous temporary values must be safe to move to another thread.

    use async_std::task;
    use std::rc::Rc;

    async fn reluctant() -> String {
        let string = Rc::new("ref-counted string".to_string());

        some_asynchronous_thing().await;

        format!("Your splendid string: {}", string)
    }

    task::spawn(reluctant());


This error message is long, but it has a lot of helpful detail:
* It explains why the future needs to be **Send: task::spawn** requires it.
* It explains which value is not **Send**: the local variable **string**, whose type is **Rc<String>**.
* It explains why **string** affects the future: it is in scope across the indicated **await**.

There are two ways to fix this problem. One is to restrict the scope of the non-**Send** value so that it doesn't cover any **await** expressions and thus doesn't need to be saved in the function's future.

    async fn reluctant() -> String {
        let return_value = {
            let string Rc::new("ref-counted string".to_string());
            format!("Your splendid string: {}", string)
            // The `Rc<String>` goes out of scope here...
        };

        // ... and thus is not around when we suspend here.
        some_asynchronous_thing().await;

        return_value
    
    }

Another solution is simply to use **std::sync::Arc** instead of **Rc**. **Arc** uses atomic updates to manage its reference counts, which makes it a bit slower, but **Arc** pointers are **Send**.

    async fn reluctant() -> String {
        let string = std::sync::Arc::new("ref-counted string".to_string());

        some_asynchronous_thing().await;

        format!("Your splendid string: {}", string)
    }

    task::spawn(reluctant());

If your future is not **Send** and you cannot conveniently make it so, then you can still use **spawn_local** to run it on the current thread. Of course, you'll need to make sure the thread calls **block_on** at some point, to give it a chance to run, and you won't benefit from distributing the work across multiple processors.

### Long Running Computations: yield_now and spawn_blocking

One way to avoid this is simply to **await** something occasionally. The **async_std::task::yield_now** function returns a simple future designed for this:

    while computation_not_done() {
        ... do one medium-sized step of computation ...
        async_std::task::yield_now().await;
    }

The first time the **yield_now** future is polled, it returns **Poll::Pending**, but says it's worth polling again soon. The effect is that your asynchronous call gives up the thread and other tasks get a chance to run, but your call with get another turn soon. The second time **yield_now**'s future is polled, it returns **Poll::Ready(())**, and your async function can resume execution.
This approach isn't always feasible, however. If you're using an external crate to do the long-running computation or calling out to C or C++, it may not be convenient to change that code to be more async-friendly. Or it may be difficult to ensure that every path through the computation is sure to hit the **await** from time to time.
For cases like this, you can use **async_std::task::spawn_blocking**. This function takes a closure, starts it running on its own thread, and returns a future of its return value. Asynchronous code can await that future, yielding its thread to other tasks until the computation is ready.

    async fn verify_password(password: &str, hash: &str, key: &str)
            -> Result<bool, argonautica::Error>
    {
        // Make copies of the arguments, so the closure can be 'static.    
        let password = password.to_string();
        let hash = hash.to_string();
        let key = key.to_string();

        async_std::task::spawn_blocking(move || {
            argonautica::Verifier::default()
                .with_hash(hash)
                .with_password(password)
                .with_secret_key(key)
                .verify()
        }).await
    }



### Comparing Asynchronous Designs

Rust's use of polling, however, is unusual. In JavaScript and C#, an asynchronous function begins running as soon as it is called, and there is a global event loop built into the system library that resumes suspended async function calls when the values they were awaiting become available.  In Rust, however, an async call does nothing until you pass it to a function like **block_on**, **spawn**, or **spawn_local** that will poll it and drive the work to tcompletion. These functions, call *executors*, play the role that other languages cover with a global event loop.
Because Rust makes you, the programmer, choose an executor to poll your futures, Rust has no need for a global event loop build into the system. The **async-std** crate offers the executor functions we've used in this chapter so far, but the **tokio** crate, which we'll use later in this chapter, defines its own set of similar executor functions. And toward the end of this chapter, we'll implement our own executor. You can use all three in the same program.



### A Real Asynchronous HTTP Client

Here's a rewrite of **many_requests**, even simpler than the one based on **cheapo_request**, that uses **surf** to run a series of requests concurrently. You'll need these dependencies in your **Cargo.toml** file:

   [dependencies]
   async-std = "1.7"
   surf = "1.0"

Then, we can define many_requests as follows:



Using a single **surf::Client** to make all our requests lets us reuse HTTP connections if several of them are directed at the same server. And no async block is needed: since **recv_string** is an asynchronous method that returns a **Send + 'static** future, we can pass its future directly to **spawn**.



## An Asynchronous Client and Server

This section's example is [a chat server and client](https://github.com/ProgrammingRust/async-chat)
In particular, we want to handle *backpressure* well. By this we mean that if one client has a slow net connection or drops its connection entirely, that must never affect other clients' ability to exchange messages at their own pace. And since a slow client should not make the server spend unbounded memory holding on to its evergrowing backlog of messages, our server should drop messages for clients that can't keep up, but notify them that their stream is incomplete. (A real chat
server would log messages to disk and let clients retrieve those they've missed, but we've left that out.)
We're depending on four crates:
* The **async-std** crate is the collection of asynchronous I/O primitives and utilities we've been using throughout the chapter.
* The **tokio** crate is another collection of asynchronous primitives like **async-std**, one of the oldest and most mature. It's widely used and holds its design and implementation to high standards, but requires a bit more care to use than **async-std**.
* The **serde** and **serde_json** crates we've seen before. These give us convenient and efficient tools for generating and parsing JSON, which our chat protocol uses to represent data on the network.

We'll present the contents of each source file over the course of the chapter, but once they're all in place, if you type **cargo build** in this tree, that compiles the library crate and then builds both executables. Cargo automatically includes the library crate as a dependency, making it a convenient place to put definitions shared by the client and server. Similarly, **cargo check** checks the entire source tree. To run either of the executables, you can
use commands like these:

    $ cargo run --release --bin server -- localhost:8088
    $ cargo run --release --bin client -- localhost:8088


### Error and Result Types

### The Protocol

### Taking User Input: Asynchronous Streams

### Sending Packets

### Receiving Packets: More Asynchronous Streams

### The Client's Main Function

### The Server's Main Function

### Handling Chat Connections: Async Mutexes

### The Group Table: Synchronous Mutexes

### Chat Groups: tokio's Broadcast Channels

## Primitive Futures and Executors: When Is a Future Worth Polling Again?

## Pinning

## When Is Asynchronous Code Helpful?


