# Asynchronous Programming

You can use Rust *asynchronous tasks* to interleave many independent activities on a single thread or a pool of worker threads. Asynchronous tasks are similar to threads, but are much quicker to create, pass control amongst themselves more efficiently, and have memory overhead an order of magnitude less than that of a thread.


## From Synchronous to Asynchronous

While this function is waiting for the system calls to return, its single thread is blocked: it can't do anything else until the sytem call finishes.

### Futures

Rust's approach to supporting asynchronous operations is to introduce a trait, **std::future::Future**:

    trait Future { 
        type Output;
        // For now, read `Pin<&mut Self>` as `&mut Self`.
        fn poll(self: Pin<&mut Self>, cx: &mut Context<'_>) 
                -> Poll<Self::Output>;
    }

    enum Poll<T> {
        Ready(T),
        Pending,
    }

A **Future** represents an operation that you can test for completion. A future's poll method never waits for the operation to finish: it always returns immediately. If the operation is complete, **poll** return **Poll::Ready(output)**, where **output** is its final result. Otherwise, it returns **Pending**. If and when the future is worth polling again, it promises to let us know by invoking a *waker*, a callback function supplied in the **Context**.

So an asynchronous version of **read_to_string** would have a signature roughly like this: 

    fn read_to_string(&mut self, buf: &mut String)
        -> impl Future<Output = Result<usize>>;

    fn read_to_string<'a>(&'a mut self, buf: &'a mut String)
        -> impl Future<Output = Result<usize>> + 'a;

One of the rules of the **Future** trait is that, once a future has returned **Poll::Ready**, it may assume it will never be polled again. Some futures just return **Poll::Pending** forever if they are overpolled; others may panic or hang.


### Async Functions and Await Expressions


    use async_std::io::prelude::*;
    use async_std::net;
    
    async fn cheapo_request(host: &str, port: u16, path: &str)
                                -> std::io::Result<String>
    {
        let mut socket = net::TcpStream::connect((host, port)).await?;
    
        let request = format!("GET {} HTTP/1.1\r\nHost: {}\r\n\r\n", path, host);
        socket.write_all(request.as_bytes()).await?;
        socket.shutdown(net::Shutdown::Write)?;
    
        let mut response = String::new();
        socket.read_to_string(&mut response).await?;
    
        Ok(response)
    }

This is token for token the same as our original version, except:

* The function starts with **async fn** instead of **fn**.
* It uses the **async_std** crate's asynchronous versions of **TcpStream::connect**, **write_all**, and **read_to_string**. These all return futures of their results. 
* After each call that returns a future, the code says **.await**. Although this looks like a reference to a struct field named **await**, it is actually special syntax built into the language for waiting until a future is ready. An **await** expression evaluates to the final value of the future. This is how the function obtains the results from **connect**, **write_all**, and **read_to_string**.

The future returned by an async function wraps up all the information the function body will need to run: the function's arguments, space for its local variables, and so on.
The future's specific type is generated automatically by the compiler, based on the function's body and arguments. This type doesn't have a name; all you know about it is that it implements **Future<Output=R>**, where **R** is the async function's return type. In this sense, futures of asynchronous functions are like closures: closures also have anonymous types, generated by the compiler, that implement the **FnOnce**, **Fn**, and **FnMut** traits.
When you first poll the future returned by **cheapo_request**, execution begins at the top of the function body and runs until the first **await** of the future returned by **TcpStream::connect**. The **await** expression polls the **connect** future, and if it is not ready, then it returns **Poll::Pending** to its own caller: polling **cheapo_request**'s future cannot preceed past that first **await** until a poll of **TcpStream::connect**'s future returns **Poll::Ready**. So a
rough equivalent of the expression **TcpStream::connect(...).await** might be:

    {
        // Note: this is pseudocode, not valid Rust
        let connect_future = TcpStream::connect(...);
        'retry_point:
        match connect_future.poll(cx) {
            Poll::Ready(value) => value,
            Poll::Pending => {
                // Arrrange for the next `poll` of `cheapo_request`'s
                // future to resume execution at 'retry_point.
                ...
                return Poll::Pending
            }
        }

    }

An **await** expression takes ownership of the future and then polls it. If it's ready, then the future's final value is the value of the **await** expression, and execution continues. Otherwise, it returns the **Poll::Pending** to its own caller.
But crucially, the next poll of cheapo_request's future doesn't start at the top of the function again: instead, it *resumes* execution mid-function at the point where it is about to poll **connect_future**. We don't progress to the rest of the async function until that future is ready.
As **cheapo_request**'s future continues to be polled, it will work its way through the function body from one **await** to the next, moving on only when the subfuture it's awaiting is ready. Thus, how many times **cheapo_request**'s future must be polled depends on both the behavior of the subfutures and the function's own control flow. **cheapo_request**'s future tracks the point at which the next **poll** should resume, and all the local state--variables, arguments,
temporaries--that resumption will need.
The ability to suspend execution mid-function and then resume later is unique to async functions.



### Calling Async Functions from Synchronous Code: block_on

We can call **cheapo_request** from an ordinary, synchronous function (like main, for example) using **async_std's task::block_on** function, which takes a future and polls it until it produces a value:


    fn main() -> std::io::Result<()> {
        use async_std::task;
    
        let response = task::block_on(cheapo_request("www.rust-lang.com", 80, "/"))?;
        println!("{}", response);
        Ok(())
    }

Since **block_on** is a synchronous function that produces the final value of an asynchronous function, you can think of it as an adapter from the asynchronous world to the synchronous world. But its blocking character also means that you should never use **block_on** within an async function: it would block the entire thread until the value is ready. Use **await** instead.

This is a good opportunity to walk through exactly what's going on in ordinary asynchronous execution:

* First, **main** calls **cheapo_request**, which returns future A of its final result. Then **main** passes that future to **async_std::block_on**, which polls it.
* Polling future A allows the body of **cheapo_request** to begin execution. It calls **TcpStream::connect** to obtain a future B of a socket and then awaits that. More precisely, since **TcpStream::connect** might encounter an error, B is a future of a **Result<TcpStream, std::io::Error>**.
* Future B gets polled by the **await**. Since the network connection is not yet established, B.poll returns **Poll::Pending**, but arranges to wake up the calling task once the socket is ready.
* Since future B wasn't ready, **A.poll** returns **Poll::Pending** to its own caller, **block_on**.
* Since **block_on** has nothing better to do, it goes to sleep. The entire thread is blocked now.
* When B's connection is ready to use, it wakes up the task that polled it. This stirs **block_on** into action, and it tries polling the future A again.
* Polling A causes **cheapo_request** to resume in its first **await**, where it polls B again.
* This time, B is ready: socket creation is complete, so it returns **Poll::Ready(Ok(socket))** to **A.poll**.
* The asynchronous call to **TcpStream::connect** is now complete. The value of the **TcpStream::connect(...).await** expression is thus **Ok(socket)**.
* The execution of **cheapo_request**'s body proceeds normally, building the request string using the **format!** macro and passing it to **socket.write_all**.
* Since **socket.write_all** is an asynchronous function, it returns a future C of its result, which **cheapo_request** duly awaits.

The rest of the story is similar.
It doesn't sound too hard to just write a loop that calls **poll** over and over. But what makes **async_std::task::block_on** valuable is that it knows how to go to sleep until the future is actually worth polling again, rather than wasting your processor time and battery life making billions of fruitless **poll** calls. The futures returned by basic I/O functions like **connect** and **read_to_string** retain the waker supplied by the **Context** passed to **poll** and invoke it when
**block_on** should wake up and try polling again.


### Spawning Async Tasks

the goal of this chapter is to get the thread *doing other work* while it's waiting.
For this, you can use **async_std::task::spawn_local**. This function takes a future and adds it to a pool that **block_on** will try polling whenever the future it's blocking on isn't ready.
The **spawn_local** function is an asynchronous analogue of the standard library's **std::thread::spawn** function for starting threads:

* **std::thread::spawn(c)** takes a closure **c** and starts a thread running it, returning a **std::thread::JoinHandle** whose **join** method waits for the thread to finish and returns whatever **c** returned.
* **async_std::task::spawn_local(f)** takes the future **f** and adds it to the pool to be polled when the current thread calls **block_on.spawn_local** returns its own **async_std::task::JoinHandle** type, itself a future that you can await to retrieve **f'**s final value.

        pub async fn many_requests(requests: Vec<(String, u16, String)>)
                            -> Vec<std::io::Result<String>>
        {
            use async_std::task;

            let mut handles = vec![];
            for (host, port, path) in requests {
                handles.push(task::spawn_local(cheapo_request(&host, port, &path)));
            }
            
            let mut results = vec![];
            for handle in handles {
                results.push(handle.await);
            }

            results

        }


All this execution takes place on a single thread, the three calls to **cheapo_request** being interleaved with each other through successive polls of their futures. An asynchronous call offers the appearance of a single function call running to completion, but this asynchronous call is realized by a series of synchronous calls to the future's **poll** method. Each individual **poll** call returns quickly, yielding the thread so that another async call can take a turn.

We have finally achieved the goal we set out at the beginning of the chapter: letting a thread take on other work while it waits for I/O to complete so that the thread's resources aren't tied up doing nothing. Even better, this goal was met with code that looks very much like ordinary Rust code: some of the functions are marked **async**, some of the function calls are followed by **.await**, and we use functions from **async_std** instead fo **std**, but otherwise,
it's ordinary Rust code.
One important difference to keep in mind between asynchronous tasks and threads is that switching from one async task to another happens only at **await** expressions, when the future being awaited returns **Poll::Pending**. This means that if you put a long-running computation in **cheapo_request**, none of the other tasks you passed to **spawn_local** will get a chance to run until it's done. With threads, this problem doesn't arise: the operating system can suspend
any thread at any point and sets timers to ensure that no thread monopolizes the processor. Asynchronous code depends on the willing cooperation of the futures sharing the thread.

### Async Blocks

Async blocks provide a concise way to separate out a section of code you'd like to run asynchronously. For example, in the previous section, **spawn_local** wrapper function to give us a future that took ownership of its arguments. You can get the same effect without the distraction of a wrapper function simply by calling **cheapo_request** from an async block:

    pub async fn many_requests(requests: Vec<(String, u16, String)>)
                    -> Vec<std::io::Result<String>>
    {
        use async_std::task;

        let mut handles = vec![];
        for (host, port, path) in requests {
            handles.push(task::spawn_local(async move {
                cheapo_request(&host, port, &path).await
            }));
        }
        ...
    }

Since this is an **async move** block, its future takes ownership of the **String** values **host** and **path**, just the way a **move** closure would. It then passes references to **cheapo_request**. The borrow checker can see that the block's **await** expression takes ownership of **cheapo_request**'s future, so the references to **host** and **path** cannot outlive the captured variables they borrow. The async block accomplishes the same thing as
**cheapo_owning_request**, but with less boilerplate.

In addition to asynchronous functions, Rust also supports *asynchronous blocks*. Whereas an ordinary block statement returns the value of its last expression, an async block returns *a future of* the value its last expression. You can use **await** expressions within an async block.


### Building Async Functions from Async Blocks

Asynchronous blocks give us another way to get the same effect as an Asynchronous function, with a little more flexibility. For example, we could write our **cheapo_request** example as an ordinary, synchronous function that returns the future of an async block(*building_async_func_from_async_blocks* fold):

    use std::io;
    use std::future::Future;

    fn cheapo_request<'a>(host: &'a str, port: u16, path: &'a str)
        -> impl Future<Output = io::Result<String>> + 'a
    {
        async move {
            ... function body ...
        }
    }

This second approach can be useful when you want to do some computation immediately when the function is called, before creating the future of its result. For example, yet another way to reconcile **cheapo_request** with **spawn_local** would be to make it into a synchronous function returning a 'static future that captures fully owned copies of its arguments(*building_async_func_from_async_blocks2* fold):

    fn cheapo_request(host: &str, port: u16, path: &str)
        -> impl Future<Output = io::Result<String>> + 'static
        {
            let host = host.to_string();
            let path = path.to_string();

            async move {
                ... use &*host, port, and path ...
            }
        }

Since this version of **cheapo_request** returns futures that are 'static, we can pass them directly to **spawn_local**: 

    let join_handle = async_std::task::spawn_local(
        cheapo_request("areweasyncyet.rs", 80, "/")
    );
    
    ... other work ...
    
    let response = join_handle.await?;

### Spawning Async Tasks on a Thread Pool

When you have enough computation to do that a single processor can't keep up, you can use **async_std::task::spawn** to spawn a future onto a pool of worker threads dedicated to polling futures that are ready to make progress.

    use async_std::task;

    let mut handles = vec![];
    for (host, port, path) in requests {
        // use async_std::task::spawn to spawn a future onto a pool of worker threads 
        // dedicated to polling futures that are ready to make progress. 
        handles.push(task::spawn(async move {
            cheapo_request(&host, port, &path).await
        }));
    }


Like **spawn_local**, **spawn** returns a **JoinHandle** value you can await to get the future's final value. But unlike **spawn_local**, the future doesn't have to wait for you to call **block_on** before it gets polled. As soon as one of the threads from the thread pool is free, it will try polling it.


### But Does Your Future Implement Send?

There is one restriction **spawn** imposes that **spawn_local** does not. Since the future is being sent off to another thread to run, the future must implement the **Send** marker trait. A future is **Send** only if all the values it contains are **Send**: all the function arguments, local variables, and even anonymous temporary values must be safe to move to another thread.

    use async_std::task;
    use std::rc::Rc;

    async fn reluctant() -> String {
        let string = Rc::new("ref-counted string".to_string());

        some_asynchronous_thing().await;

        format!("Your splendid string: {}", string)
    }

    task::spawn(reluctant());


This error message is long, but it has a lot of helpful detail:
* It explains why the future needs to be **Send: task::spawn** requires it.
* It explains which value is not **Send**: the local variable **string**, whose type is **Rc<String>**.
* It explains why **string** affects the future: it is in scope across the indicated **await**.

There are two ways to fix this problem. One is to restrict the scope of the non-**Send** value so that it doesn't cover any **await** expressions and thus doesn't need to be saved in the function's future.

    async fn reluctant() -> String {
        let return_value = {
            let string Rc::new("ref-counted string".to_string());
            format!("Your splendid string: {}", string)
            // The `Rc<String>` goes out of scope here...
        };

        // ... and thus is not around when we suspend here.
        some_asynchronous_thing().await;

        return_value
    
    }

Another solution is simply to use **std::sync::Arc** instead of **Rc**. **Arc** uses atomic updates to manage its reference counts, which makes it a bit slower, but **Arc** pointers are **Send**.

    async fn reluctant() -> String {
        let string = std::sync::Arc::new("ref-counted string".to_string());

        some_asynchronous_thing().await;

        format!("Your splendid string: {}", string)
    }

    task::spawn(reluctant());

If your future is not **Send** and you cannot conveniently make it so, then you can still use **spawn_local** to run it on the current thread. Of course, you'll need to make sure the thread calls **block_on** at some point, to give it a chance to run, and you won't benefit from distributing the work across multiple processors.

### Long Running Computations: yield_now and spawn_blocking

One way to avoid this is simply to **await** something occasionally. The **async_std::task::yield_now** function returns a simple future designed for this:

    while computation_not_done() {
        ... do one medium-sized step of computation ...
        async_std::task::yield_now().await;
    }

The first time the **yield_now** future is polled, it returns **Poll::Pending**, but says it's worth polling again soon. The effect is that your asynchronous call gives up the thread and other tasks get a chance to run, but your call with get another turn soon. The second time **yield_now**'s future is polled, it returns **Poll::Ready(())**, and your async function can resume execution.
This approach isn't always feasible, however. If you're using an external crate to do the long-running computation or calling out to C or C++, it may not be convenient to change that code to be more async-friendly. Or it may be difficult to ensure that every path through the computation is sure to hit the **await** from time to time.
For cases like this, you can use **async_std::task::spawn_blocking**. This function takes a closure, starts it running on its own thread, and returns a future of its return value. Asynchronous code can await that future, yielding its thread to other tasks until the computation is ready.

    async fn verify_password(password: &str, hash: &str, key: &str)
            -> Result<bool, argonautica::Error>
    {
        // Make copies of the arguments, so the closure can be 'static.    
        let password = password.to_string();
        let hash = hash.to_string();
        let key = key.to_string();

        async_std::task::spawn_blocking(move || {
            argonautica::Verifier::default()
                .with_hash(hash)
                .with_password(password)
                .with_secret_key(key)
                .verify()
        }).await
    }



### Comparing Asynchronous Designs

Rust's use of polling, however, is unusual. In JavaScript and C#, an asynchronous function begins running as soon as it is called, and there is a global event loop built into the system library that resumes suspended async function calls when the values they were awaiting become available.  In Rust, however, an async call does nothing until you pass it to a function like **block_on**, **spawn**, or **spawn_local** that will poll it and drive the work to tcompletion. These functions, call *executors*, play the role that other languages cover with a global event loop.
Because Rust makes you, the programmer, choose an executor to poll your futures, Rust has no need for a global event loop build into the system. The **async-std** crate offers the executor functions we've used in this chapter so far, but the **tokio** crate, which we'll use later in this chapter, defines its own set of similar executor functions. And toward the end of this chapter, we'll implement our own executor. You can use all three in the same program.



### A Real Asynchronous HTTP Client

Here's a rewrite of **many_requests**, even simpler than the one based on **cheapo_request**, that uses **surf** to run a series of requests concurrently. You'll need these dependencies in your **Cargo.toml** file:

   [dependencies]
   async-std = "1.7"
   surf = "1.0"

Then, we can define many_requests as follows:



Using a single **surf::Client** to make all our requests lets us reuse HTTP connections if several of them are directed at the same server. And no async block is needed: since **recv_string** is an asynchronous method that returns a **Send + 'static** future, we can pass its future directly to **spawn**.



## An Asynchronous Client and Server

This section's example is [a chat server and client](https://github.com/ProgrammingRust/async-chat)
In particular, we want to handle *backpressure* well. By this we mean that if one client has a slow net connection or drops its connection entirely, that must never affect other clients' ability to exchange messages at their own pace. And since a slow client should not make the server spend unbounded memory holding on to its evergrowing backlog of messages, our server should drop messages for clients that can't keep up, but notify them that their stream is incomplete. (A real chat
server would log messages to disk and let clients retrieve those they've missed, but we've left that out.)
We're depending on four crates:
* The **async-std** crate is the collection of asynchronous I/O primitives and utilities we've been using throughout the chapter.
* The **tokio** crate is another collection of asynchronous primitives like **async-std**, one of the oldest and most mature. It's widely used and holds its design and implementation to high standards, but requires a bit more care to use than **async-std**.
* The **serde** and **serde_json** crates we've seen before. These give us convenient and efficient tools for generating and parsing JSON, which our chat protocol uses to represent data on the network.

in addition to the main library crate, *src/lib.rs*, with its submodule *src/utils.rs*, it also includes two executables:

* *src/bin/client.rs* is a single-file executable for the chat client.
* *src/bin/server* is the server executable, spread across four files: *main.rs* holds the **main** function, and there are three submodules, *connection.rs*, *group.rs*, and *group_table.rs*.

We'll present the contents of each source file over the course of the chapter, but once they're all in place, if you type **cargo build** in this tree, that compiles the library crate and then builds both executables. Cargo automatically includes the library crate as a dependency, making it a convenient place to put definitions shared by the client and server. Similarly, **cargo check** checks the entire source tree. To run either of the executables, you can
use commands like these:

    $ cargo run --release --bin server -- localhost:8088
    $ cargo run --release --bin client -- localhost:8088

The **--bin** option indicates which executable to run, and any arguments following the -- option get passed to the executable itself. Our client and server just want to know the server's address and TCP port.

### Error and Result Types

    use std::error::Error;

    pub type ChatError = Box<dyn Error + Send + Sync + 'static>;
    pub type ChatResult<T> = Result<T, ChatError>;

These are the general-purpose error types we suggested in "Working with Multiple Error Types" on page 166. The **async_std**, **serde_json**, and **tokio** crates each define their own error types, but the **?** operator can automatically convert them all into a **ChatError**, using the standard library's implementation of the **From** trait that can convert any suitable error type to **Box<dyn Error + Send + Sync + 'static>. The **Send** and **Sync** bounds ensure
that if a task spawned onto another thread fails, it can safely report the error to the main thread.
In a real application, consider using the **anyhow** crate, which provides **Error** and **Result** types similar to these. The **anyhow** crate is easy to use and provides some nice features beyond what our **ChatError** and **ChatResult** can offer.


### The Protocol

    use serde::{Deserialize, Serialize};
    use std::sync::Arc;
    
    pub mod utils;
    
    #[derive(Debug, Deserialize, Serialize, PartialEq)]
    pub enum FromClient {
        Join { group_name: Arc<String> },
        Post {
            group_name: Arc<String>,
            message: Arc<String>,
        },
    }
    
    #[derive(Debug, Deserialize, Serialize, PartialEq)]
    pub enum FromServer {
        Message {
            group_name: Arc<String>,
            message: Arc<String>,
        },
        Error(String),
    }
    
    #[test]
    fn test_fromclient_json() {
        use std::sync::Arc;
    
        let from_client = FromClient::Post {
            group_name: Arc::new("Dogs".to_string()),
            message: Arc::new("Samoyeds rock!".to_string()),
        };
    
        let json = serde_json::to_string(&from_client).unwrap();
        assert_eq!(json,
                r#"{"Post":{"group_name":"Dogs","message":"Samoyeds rock!"}}"#);
    
        assert_eq!(serde_json::from_str::<FromClient>(&json).unwrap(),
                from_client);
    }


The **FromClient** enum represents the packets a client can send to the server: it can ask to join a room and post messages to any room it has joined. **FromServer** represents what the server can send back: messages posted to some group, and error messages. Using a reference-counted **Arc<String>** instead of a plain **String** helps the server avoid making copies of strings as it manages groups and distributes messages.

Note that the **Arc** pointers in **FromClient** have no effect on the serialized form: the reference-counted strings appear directly as JSON object member values.

### Taking User Input: Asynchronous Streams

The asynchronous **BufReader**'s lines method is interesting. It can't return an iterator, the way the standard library does: the **Iterator::next** method is an ordinary synchronous function, so calling **commands.next()** would block the thread until the next line was ready. Instead, **lines** returns a *stream* of **Result<String>** values. A stream is the asynchronous analogue of an iterator: it produces a sequence of values on demand, in an async-friendly fashion. Here's the
definition of the **Stream** trait, from the **async_std::stream** module:

    trait Stream {
        type Item;
    
        // For now, read `Pin<&mut Self>` as `&mut Self`.
        fn poll_next(self: Pin<&mut Self>, cx: &mut Context<'_>)
            -> Poll<Option<Self::Item>>;
    }

You can look at this as a hybrid of the **Iterator** and **Future** traits. Like an iterator, a **Stream** has an associated **Item** type and uses **Option** to indicate when the sequence has ended. But like a future, a stream must be polled: to get the next item (or learn that the stream has ended), you must call **poll_next** until it returns **Poll::Ready**. A stream's **poll_next** implementation should always return quickly, without blocking. And if a stream returns
**Poll::Pending**, it must notify the caller when it's worth polling again via the **Context**.
The **poll_next** method is awkward to use directly, but you won't generally need to do that. Like iterators, streams have a broad collection of utility methods like **filter** and **map**. Among these is a **next** method, which returns a future of the stream's next **Option<Self::Item>**. Rather than polling the stream explicitly, you can call **next** and await the future it returns instead.


When working with streams, it's important to remember to use the **async_std** prelude:

    use async_std::prelude::*;

    pub trait StreamExt: Stream {
        ... define utility methods as default methods ...
    }

    impl<T: Stream> StreamExt for T {}

This is an example of the *extension trait* pattern we described in "Traits and Other People's Types". The **async_std::prelude** module brings the **StreamExt** methods into scope, so using the prelude ensures its methods are visible in your code.


### Sending Packets

For transmitting packets on a network socket, our client and server use the **send_as_json** function from our library crate's **utils** module:

    use async_std::prelude::*;
    use serde::Serialize;
    use std::marker::Unpin;

    pub async fn send_as_json<S, P>(outbound: &mut S, packet: &P) -> ChatResult<()>
    where
        S: async_std::io::Write + Unpin,
        P: Serialize,
    {
        let mut json = serde_json::to_string(&packet)?;
        json.push('\n');
        outbound.write_all(json.as_bytes()).await?;
        Ok(())
    }

This function builds the JSON representation of packet as a String, adds a newline to the end, and then writes it all to outbound.

As with streams, many of the methods of **async_std**'s I/O traits are actually defined on extension traits, so it's important to remember to use **async_std::prelude::*** whenever you are using them.


### Receiving Packets: More Asynchronous Streams

    use serde::de::DeserializeOwned;

    pub fn receive_as_json<S, P>(inbound: S) -> impl Stream<Item = ChatResult<P>>
        where S: async_std::io::BufRead + Unpin,
              P: DeserializeOwned,
    {
        inbound.lines()
        .map(|line_result| -> ChatResult<P> {
            let line = line_result?;
            let parsed = serde_json::from_str::<P>(&line)?;
            Ok(parsed)
        })
    }

Like **send_as_json**, this function is generic in the input stream and packet types:

* The stream type S must implement **async_std::io::BufRead**, the asynchronous analogue of **std::io::BufRead**, representing a buffered input byte stream.
* The packet type P must implement **DeserializeOwned**, a stricter variant of **serde's Deserialize** trait. 

Notice that **receive_as_json** is not, itself, an asynchronous function. It is an ordinary function that returns an async value, a stream.
To see how **receive_as_json** gets used, here is our chat client's **handle_replies** function from *src/bin/client.rs*, which receives a stream of **FromServer** values from the network and prints them out for the user to see:

    use async_chat::FromServer;

    async fn handle_replies(from_server: net::TcpStream) -> ChatResult<()> {
        let buffered = io::BufReader::new(from_server);
        let mut reply_stream = utils::receive_as_json(buffered);

        while let Some(reply) = reply_stream.next().await {
            match reply? {
                FromServer::Message { group_name, message } => {
                    println!("message posted to {}: {}", group_name, message);
                }
                FromServer::Error(message) => {
                    println!("error from server: {}", message);
                }
            }
        }

        Ok(())
    }

### The Client's Main Function

we can show the chat client's main function, from *src/bin/client.rs*:

    use async_std::task;

    fn main() -> ChatResult<()> {
        let address = std::env::args().nth(1)
            .expect("Usage: client ADDRESS:PORT");

        task::block_on(async {
            let socket = net::TcpStream::connect(address).await?;
            socket.set_nodelay(true)?;

            let to_server = send_comands(socket.clone());
            let from_server = handle_replies(socket);

            from_server.race(to_server).await?;

            Ok(())
        })
    }

Having obtained the server's address from the command line, **main** has a series of asynchronous functions it would like to call, so it wraps the remainder of the function in an asynchronous block and passes the block's future to **async_std::task::block_on** to run.


### The Server's Main Function

Here are the entire contents of the main file for the server, *src/bin/server/main.rs*:


    use async_std::prelude::*;
    use async_chat::utils::ChatResult;
    use std::sync::Arc;
    
    mod connection;
    mod group;
    mod group_table;
    
    use connection::serve;
    
    fn main() -> ChatResult<()> {
        let address = std::env::args().nth(1).expect("Usage: server ADDRESS");
    
        let chat_group_table = Arc::new(group_table::GroupTable::new());
    
        async_std::task::block_on(async {
            // This code was shown in the chapter introduction.
            use async_std::{net, task};
    
            let listener = net::TcpListener::bind(address).await?;
    
            let mut new_connections = listener.incoming();
            while let Some(socket_result) = new_connections.next().await {
                let socket = socket_result?;
                let groups = chat_group_table.clone();
                task::spawn(async {
                    log_error(serve(socket, groups).await);
                });
            }
    
            Ok(())
        })
    }
    
    fn log_error(result: ChatResult<()>) {
        if let Err(error) = result {
            eprintln!("Error: {}", error);
        }
    }

The server's **main** function resembles the client's: it does a little bit of setup and then calls **block_on** to run an async block that does the real work. To handle incoming connections from clients, it creates a **TcpListener** socket, whose **incoming** method returns a stream of **std::io::Result<TcpStream>** values.

### Handling Chat Connections: Async Mutexes

Here's the server's workhorse: the **serve** function from the **connection** module in *src/bin/server/connection.rs*:

    use async_chat::{FromClient, FromServer};
    use async_chat::utils::{self, ChatResult};
    use async_std::prelude::*;
    use async_std::io::BufReader;
    use async_std::net::TcpStream;
    use async_std::sync::Arc;
    
    use crate::group_table::GroupTable;
    
    pub async fn serve(socket: TcpStream, groups: Arc<GroupTable>)
                    -> ChatResult<()>
    {
        let outbound = Arc::new(Outbound::new(socket.clone()));
    
        let buffered = BufReader::new(socket);
        let mut from_client = utils::receive_as_json(buffered);
        while let Some(request_result) = from_client.next().await {
            let request = request_result?;
    
            let result = match request {
                FromClient::Join { group_name } => {
                    let group = groups.get_or_create(group_name);
                    group.join(outbound.clone());
                    Ok(())
                }
    
                FromClient::Post { group_name, message } => {
                    match groups.get(&group_name) {
                        Some(group) => {
                            group.post(message);
                            Ok(())
                        }
                        None => {
                            Err(format!("Group '{}' does not exist", group_name))
                        }
                    }
                }
            };
            if let Err(message) = result {
                let report = FromServer::Error(message);
                outbound.send(report).await?;
            }
        }
    
        Ok(())
    }
    
This is almost a mirror image of the client's **handle_replies** function: the bulk of the code is a loop handling an incoming stream of **FromClient** values, built from a buffered TCP stream with **receive_as_json**. If an error occurs, we generate a **FromServer::Error** packet to convey the bad news back to the client.

This is managed with the **Outbound** type, defined in *src/bin/server/connection.rs* as follows:

    use async_std::sync::Mutex;
    
    pub struct Outbound(Mutex<TcpStream>);
    
    impl Outbound {
        pub fn new(to_client: TcpStream) -> Outbound {
            Outbound(Mutex::new(to_client))
        }
    
        pub async fn send(&self, packet: FromServer) -> ChatResult<()> {
            let mut guard = self.0.lock().await;
            utils::send_as_json(&mut *guard, &packet).await?;
            guard.flush().await?;
            Ok(())
        }
    }

When created, an **Outbound** value takes ownership of a **TcpStream** and wraps it in a Mutex to ensure that only one task can use it at a time. The **serve** function wraps each **Outbound** in an Arc reference-counted pointer so that all the groups the client joins can point to the same shared **Outbound** instance.

Note that **Outbound** uses the **async_std::sync::Mutex** type, not the standard library's **Mutex**. There are three reasons for this.

First, the standard library's **Mutex** may misbehave if a task is suspended while holding a mutex guard. If the thread that had been running that task picks up another task that tries to lock the same **Mutex**, trouble ensures: from the **Mutex**'s point of view, the thread that already owns it is trying to lock it again. The standard **Mutex** isn't designed to handle this case, so it panics or deadlocks. (It will never grant the lock inappropriately.) There is work
underway to make Rust detect this problem at compile time and issue a warning whenever a **std::sync::Mutex** guard is live across an **await** expression. Since **Outbound::send** needs to hold the lock while it awaits the futures of send_as_json and guard.flush, it must use async_std's Mutex.

 

### The Group Table: Synchronous Mutexes

Often there is no need to await anything while holding a mutex, and the lock is not held for long. In such cases, the standard library's Mutex can be much more efficient.

    use crate::group::Group;
    use std::collections::HashMap;
    use std::sync::{Arc, Mutex};
    
    pub struct GroupTable(Mutex<HashMap<Arc<String>, Arc<Group>>>);
    
    impl GroupTable {
        pub fn new() -> GroupTable {
            GroupTable(Mutex::new(HashMap::new()))
                
        }
    
        pub fn get(&self, name: &String) -> Option<Arc<Group>> {
            self.0.lock()
                .unwrap()
                .get(name)
                .cloned()
        }
    
        pub fn get_or_create(&self, name: Arc<String>) -> Arc<Group> {
            self.0.lock()
                .unwrap()
                .entry(name.clone())
                .or_insert_with(|| Arc::new(Group::new(name)))
                .clone()
        }
    }

A **GroupTable** is simple a mutex-protected hash table, mapping chat group names to actual groups, both managed using reference-counted pointers. The **get** and **get_or_create** methods lock the mutex, perform a few hash table operations, perhaps some allocations, and return.

If our chat server found itself with millions of users, and the **GroupTable** mutex did become a bottleneck, making it asynchronous wouldn't address that problem. It would probably be better to use some sort of collection type specialized for concurrent access instead of **HashMap**. For example, the **dashmap** crate provides such a type.


### Chat Groups: tokio's Broadcast Channels



## Primitive Futures and Executors: When Is a Future Worth Polling Again?

### Invoking Wakers: spawn_blocking

### Implementing block_on

## Pinning

### The Two Life Stages of a Future

### Pinned Pointers

### The Unpin Trait

## When Is Asynchronous Code Helpful?


